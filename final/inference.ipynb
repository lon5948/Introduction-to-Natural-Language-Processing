{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPPhx4owgk3mMehzEi2fizo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7uk9KXUSCziU","executionInfo":{"status":"ok","timestamp":1673545982073,"user_tz":-480,"elapsed":18299,"user":{"displayName":"紀竺均","userId":"10396067312214412745"}},"outputId":"38012a02-f54a-4f02-cc99-7f5aa18f5a40"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["!pip install transformers==4.15.0"],"metadata":{"id":"qmBrNi95C_Jb","executionInfo":{"status":"ok","timestamp":1673545991338,"user_tz":-480,"elapsed":9269,"user":{"displayName":"紀竺均","userId":"10396067312214412745"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d36d9ef9-15c4-4b60-f217-a7359b7ed704"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.15.0\n","  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (2.25.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.15.0) (3.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.15.0) (3.0.9)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.15.0) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.15.0) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.15.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.15.0) (2022.12.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.15.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.15.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.15.0) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=99e7ad884f6dbef6fab6eab18932f3503a3389f0197938891daae9721c4e96b2\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.15.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from transformers import RobertaTokenizer\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import RobertaTokenizer, RobertaModel, AdamW\n","import numpy as np\n","import shutil\n","from tqdm import tqdm\n","from configparser import ConfigParser\n","import os\n","import json\n","import argparse\n","import shutil\n","import sys\n","import transformers\n","from nltk.tokenize import sent_tokenize\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import warnings\n","warnings.simplefilter(\"ignore\")"],"metadata":{"id":"Lg5wB2iJFnp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_dataframes():\n","    df = {}\n","    df[\"train\"] = pd.read_pickle(\"data/train.pkl\")\n","    df[\"valid\"] = pd.read_pickle(\"data/valid.pkl\")\n","    df[\"test\"] = pd.read_pickle(\"data/test.pkl\")\n","    return df\n","\n","\n","class ClaimData(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe['text']\n","        self.evidence = dataframe['evidence_sents']\n","        self.targets = dataframe['rating']\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        text = \" \".join(text.split())\n","\n","        evidence = str(self.evidence[index])\n","        evidence = \" \".join(evidence.split())\n","\n","        inputs = self.tokenizer(\n","            text,\n","            evidence,\n","            truncation=\"longest_first\",\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'labels': torch.tensor(self.targets[index], dtype=torch.long)\n","        }\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    accuracy = accuracy_score(labels, predictions)\n","    target_names = [0, 1, 2]\n","    output_dict = classification_report(labels, predictions, labels=range(3), target_names=target_names, output_dict=True)\n","    return {'accuracy': accuracy, 'macro f1-score': output_dict['macro avg']['f1-score']}\n","\n","\n"],"metadata":{"id":"Ws8Onu_PD2-f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rand_seed = 42\n","torch.manual_seed(rand_seed)\n","num_epochs = 3\n","output_dir = \"output\"\n","if not os.path.isdir(output_dir):\n","    os.makedirs(output_dir)\n","\n","df = load_dataframes()\n","\n","model_checkpoint = \"output/checkpoint-5620\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","MAX_SEQ_LEN = 512\n","print(\"creating data....\")\n","training_set = ClaimData(df['train'], tokenizer, MAX_SEQ_LEN)\n","validation_set = ClaimData(df['valid'], tokenizer, MAX_SEQ_LEN)\n","testing_set = ClaimData(df['test'], tokenizer, MAX_SEQ_LEN)\n","print(\"setting model....\")\n","model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n","\n","args = TrainingArguments(\n","    output_dir = output_dir,\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = 'epoch',\n","    learning_rate=5e-6,\n","    per_device_train_batch_size=6,\n","    per_device_eval_batch_size=6,\n","    num_train_epochs=num_epochs,\n","    weight_decay=0.001,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='macro f1-score',\n","    seed=rand_seed\n",")\n","\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=training_set,\n","    eval_dataset=validation_set,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"],"metadata":{"id":"34J1mW2aETW8","executionInfo":{"status":"ok","timestamp":1673546400941,"user_tz":-480,"elapsed":16994,"user":{"displayName":"紀竺均","userId":"10396067312214412745"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"886202aa-be77-484b-8139-ed0c6cf1eacc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Didn't find file output/checkpoint-5620/added_tokens.json. We won't load it.\n","loading file output/checkpoint-5620/vocab.json\n","loading file output/checkpoint-5620/merges.txt\n","loading file output/checkpoint-5620/tokenizer.json\n","loading file None\n","loading file output/checkpoint-5620/special_tokens_map.json\n","loading file output/checkpoint-5620/tokenizer_config.json\n","loading configuration file output/checkpoint-5620/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"output/checkpoint-5620\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file output/checkpoint-5620/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["creating data....\n","setting model....\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at output/checkpoint-5620.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","source":["predictions, labels, metrics = trainer.predict(testing_set)\n","metrics[\"predict_samples\"] = len(testing_set)\n","\n","trainer.log_metrics(\"predict\", metrics)\n","trainer.save_metrics(\"predict\", metrics)\n","\n","predictions = np.argmax(predictions, axis=1)\n","output_predict_file = os.path.join(output_dir, \"predictions.csv\")"],"metadata":{"id":"Qa1U6xI6Ec2e","executionInfo":{"status":"ok","timestamp":1673546566496,"user_tz":-480,"elapsed":148756,"user":{"displayName":"紀竺均","userId":"10396067312214412745"}},"colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"e82e5b1a-36a9-4de8-beb4-2ade10639797"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 2357\n","  Batch size = 6\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='393' max='393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [393/393 02:27]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["***** predict metrics *****\n","  predict_samples         =       2357\n","  test_accuracy           =     0.3975\n","  test_loss               =     1.2556\n","  test_macro f1-score     =     0.1896\n","  test_runtime            = 0:02:28.11\n","  test_samples_per_second =     15.913\n","  test_steps_per_second   =      2.653\n"]}]},{"cell_type":"code","source":["test_ids = df[\"test\"][\"claim_id\"].values\n","import csv\n","if trainer.is_world_process_zero():\n","    with open(output_predict_file, \"w\") as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\"id\", \"rating\"])\n","        for index, item in zip(test_ids, predictions):\n","            writer.writerow([index,item])\n","        writer.writerow([30380, 0])\n","        writer.writerow([31035, 0])\n","        writer.writerow([31420, 0])"],"metadata":{"id":"IIBs7zuzDYZp"},"execution_count":null,"outputs":[]}]}