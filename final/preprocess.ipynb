{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORJpu3m0eJ2fcG99FiQ83a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"X70NNwU_6nd-"},"outputs":[],"source":["import pandas as pd\n","import json\n","import os\n","from tqdm import tqdm\n","import string\n","from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","source":["def load_articles(dataset_path):\n","\n","    with open(os.path.join(dataset_path, \"train.json\")) as f_obj:\n","        train_records = json.load(f_obj)\n","\n","    with open(os.path.join(dataset_path, \"valid.json\")) as f_obj:\n","        valid_records = json.load(f_obj)\n","\n","    with open(os.path.join(dataset_path, \"test.json\")) as f_obj:\n","        test_records = json.load(f_obj)\n","\n","    all_records = train_records + valid_records + test_records\n","\n","    relevant_article_paths = []\n","    for record in all_records:\n","        for _, article_path in record[\"metadata\"][\"premise_articles\"].items():\n","            relevant_article_paths.append(article_path)\n","\n","    rel_article_path_to_sents = {}\n","    all_rel_sentences = []\n","    for rel_article_path in tqdm(relevant_article_paths):\n","        with open(os.path.join(\"articles\", rel_article_path)) as f_obj:\n","            article_sentences = json.load(f_obj)\n","        rel_article_path_to_sents[rel_article_path] = article_sentences\n","        all_rel_sentences.extend(article_sentences)\n","\n","    return (\n","        rel_article_path_to_sents,\n","        all_rel_sentences\n","    )"],"metadata":{"id":"IsMWfozx6vu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_sentence(sentence):\n","    sentence = sentence.lower()\n","    # remove link, website or short keywords\n","    if ('link:' in sentence) or (r'http' in sentence) or len(sentence.split()) <= 4:\n","        return \"\"\n","    # remove punctuation\n","    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n","    # remove additional space\n","    sentence = sentence.replace(r'\\s+', ' ')\n","    # print(sentence)\n","    return sentence"],"metadata":{"id":"u-pRjmEN62Dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_data(dataset_path, article_path_to_sents, tfidf_vectorizer):\n","    with open(os.path.join(dataset_path, \"train.json\")) as f_obj:\n","        train_records = json.load(f_obj)\n","\n","    train = []\n","    for record in tqdm(train_records):\n","        train_item = {}\n","\n","        train_item[\"claim_id\"] = record[\"metadata\"][\"id\"]\n","\n","        if record[\"metadata\"][\"claimant\"] is not None:\n","            train_item[\"text\"] = (\n","                record[\"metadata\"][\"claim\"].strip()\n","                + \" \"\n","                + record[\"metadata\"][\"claimant\"].strip()\n","            )\n","            train_item[\"text\"] = train_item[\"text\"].strip()\n","        else:\n","            train_item[\"text\"] = record[\"metadata\"][\"claim\"].strip()\n","\n","        evidence_sentences = []\n","\n","        for _, article_path in record[\"metadata\"][\"premise_articles\"].items():\n","            if article_path not in article_path_to_sents:\n","                continue\n","\n","            article_sentences = []\n","            for sent in article_path_to_sents[article_path]:\n","                sent = preprocess_sentence(sent)\n","                if sent.strip() != \"\":\n","                    article_sentences.append(sent.strip())\n","\n","            if len(article_sentences) == 0:\n","                continue\n","\n","            Y = tfidf_vectorizer.transform(article_sentences)\n","\n","            evidence_sentences.append(article_sentences)\n","\n","        if len(evidence_sentences) == 0:\n","            continue\n","\n","        train_item[\"evidence_sents\"] = evidence_sentences\n","        train_item[\"rating\"] = record[\"label\"][\"rating\"]\n","        train.append(train_item)\n","\n","    train_df = pd.DataFrame.from_records(train)\n","    print(train_df)\n","\n","    with open(os.path.join(dataset_path, \"valid.json\")) as f_obj:\n","        valid_records = json.load(f_obj)\n","\n","    valid = []\n","    for record in tqdm(valid_records):\n","        valid_item = {}\n","\n","        valid_item[\"claim_id\"] = record[\"metadata\"][\"id\"]\n","\n","        if record[\"metadata\"][\"claimant\"] is not None:\n","            valid_item[\"text\"] = (\n","                record[\"metadata\"][\"claim\"].strip()\n","                + \" \"\n","                + record[\"metadata\"][\"claimant\"].strip()\n","            )\n","            valid_item[\"text\"] = valid_item[\"text\"].strip()\n","        else:\n","            valid_item[\"text\"] = record[\"metadata\"][\"claim\"].strip()\n","\n","        evidence_sentences = []\n","        evidence_scores = []\n","        for _, article_path in record[\"metadata\"][\"premise_articles\"].items():\n","            \n","            if article_path not in article_path_to_sents:\n","                continue\n","\n","            article_sentences = []\n","            for sent in article_path_to_sents[article_path]:\n","                sent = preprocess_sentence(sent)\n","                if sent.strip() != \"\":\n","                    article_sentences.append(sent.strip())\n","\n","            if len(article_sentences) == 0:\n","                continue\n","\n","            Y = tfidf_vectorizer.transform(article_sentences)\n","\n","            evidence_sentences.append(article_sentences)\n","\n","        if len(evidence_sentences) == 0:\n","            continue\n","\n","        valid_item[\"evidence_sents\"] = evidence_sentences\n","        valid_item[\"rating\"] = record[\"label\"][\"rating\"]\n","        valid.append(valid_item)\n","\n","    valid_df = pd.DataFrame.from_records(valid)\n","    print(valid_df)\n","\n","    with open(os.path.join(dataset_path, \"test.json\")) as f_obj:\n","        test_records = json.load(f_obj)\n","\n","    test = []\n","    for record in tqdm(test_records):\n","        test_item = {}\n","\n","        test_item[\"claim_id\"] = record[\"metadata\"][\"id\"]\n","\n","        if record[\"metadata\"][\"claimant\"] is not None:\n","            test_item[\"text\"] = (\n","                record[\"metadata\"][\"claim\"].strip()\n","                + \" \"\n","                + record[\"metadata\"][\"claimant\"].strip()\n","            )\n","            test_item[\"text\"] = test_item[\"text\"].strip()\n","        else:\n","            test_item[\"text\"] = record[\"metadata\"][\"claim\"].strip()\n","\n","        evidence_sentences = []\n","        for _, article_path in record[\"metadata\"][\"premise_articles\"].items():\n","            if article_path not in article_path_to_sents:\n","                continue\n","\n","            article_sentences = []\n","            for sent in article_path_to_sents[article_path]:\n","                sent = preprocess_sentence(sent)\n","                if sent.strip() != \"\":\n","                    article_sentences.append(sent.strip())\n","\n","            if len(article_sentences) == 0:\n","                continue\n","\n","            Y = tfidf_vectorizer.transform(article_sentences)\n","\n","\n","            evidence_sentences.append(article_sentences)\n","\n","        if len(evidence_sentences) == 0:\n","            continue\n","\n","        test_item[\"evidence_sents\"] = evidence_sentences\n","        test_item[\"rating\"] = 0\n","        test.append(test_item)\n","    test_df = pd.DataFrame.from_records(test)\n","    return train_df, valid_df, test_df"],"metadata":{"id":"hRtSLWUQ739T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_path = \"./data\"\n","print(\"Loading articles...\")\n","(\n","  rel_article_path_to_sents,\n","  all_rel_sentences\n",") = load_articles(dataset_path)\n","  \n","print(\"Generating tf idf vectorizer...\")\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(all_rel_sentences)\n","\n","print(\"Generating dataframes with relevant articles...\")\n","train_w, valid_w, test_w = load_data(\n","  dataset_path, rel_article_path_to_sents, vectorizer\n",")\n","\n","train_w.to_pickle(os.path.join(dataset_path, \"train.pkl\"))\n","valid_w.to_pickle(os.path.join(dataset_path, \"valid.pkl\"))\n","test_w.to_pickle(os.path.join(dataset_path, \"test.pkl\"))"],"metadata":{"id":"MC9tWGuG8BIc"},"execution_count":null,"outputs":[]}]}