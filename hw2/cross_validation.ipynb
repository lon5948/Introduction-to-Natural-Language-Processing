{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(label):\n",
    "    if label == \"neutral\":\n",
    "        return 0\n",
    "    elif label == \"anger\":\n",
    "        return 1\n",
    "    elif label == \"joy\":\n",
    "        return 2\n",
    "    elif label == \"surprise\":\n",
    "        return 3\n",
    "    elif label == \"sadness\":\n",
    "        return 4\n",
    "    elif label == \"disgust\":\n",
    "        return 5\n",
    "    elif label == \"fear\":\n",
    "        return 6\n",
    "    \n",
    "def encode(text, word2index, label, N):\n",
    "    tokenized = word_tokenize(text)\n",
    "    encoded = [0]*N\n",
    "    enc1 = [word2index.get(word) for word in tokenized]\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return (encoded,label)\n",
    "\n",
    "def encode_test(text, word2index, N):\n",
    "    tokenized = word_tokenize(text)\n",
    "    for i,word in enumerate(tokenized):\n",
    "        if word2index.get(word)==None:\n",
    "            tokenized[i]='unk'\n",
    "\n",
    "    encoded = [0]*N\n",
    "    enc1 = [word2index.get(word) for word in tokenized]\n",
    "\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, target)\n",
    "        _, pred = torch.max(preds, 1)\n",
    "        acc = accuracy_score(pred.tolist(), target.tolist())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, target)\n",
    "        _, pred = torch.max(preds, 1)\n",
    "        acc = accuracy_score(pred.tolist(), target.tolist())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(sample_size, k=5):\n",
    "    # calculate the size of each fold\n",
    "    fold_size = np.ones(k , dtype=int) * (sample_size // k) \n",
    "    fold_size[:sample_size % k] += 1 # if the sample size is not divisible by k\n",
    "    # shuffle\n",
    "    indexs = np.arange(sample_size)\n",
    "    np.random.default_rng(seed=10).shuffle(indexs)\n",
    "    # split indexs  \n",
    "    folds = np.array_split(indexs, k)\n",
    "    folds = np.array(folds)\n",
    "    # return training index and val index in each fold\n",
    "    ret = []\n",
    "    for i in range(k):\n",
    "        train_fold_ind = np.delete(np.arange(k), i)\n",
    "        train_fold = np.concatenate((folds[train_fold_ind]), axis=None) \n",
    "        val_fold = folds[i]\n",
    "        ret.append([train_fold, val_fold])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_HW2dataset.csv')\n",
    "dev_df = pd.read_csv('dev_HW2dataset.csv')\n",
    "\n",
    "train_df = train_df[['Emotion','Utterance']]\n",
    "dev_df = dev_df[['Emotion','Utterance']]\n",
    "\n",
    "train_set = list(train_df.to_records(index=False))\n",
    "dev_set = list(dev_df.to_records(index=False))\n",
    "\n",
    "counts = Counter()\n",
    "for ds in [train_set, dev_set]:\n",
    "    for label,text in ds:\n",
    "        counts.update(word_tokenize(text))\n",
    "\n",
    "word2index = {'unk':0}\n",
    "for i,word in enumerate(counts.keys()):\n",
    "    word2index[word] = i+1\n",
    "index2word = {v:k for k,v in word2index.items()}\n",
    "\n",
    "train_set = train_set + dev_set\n",
    "train_encoded = [(encode(Utterance,word2index,label_map(label),12)) for label, Utterance in train_set]\n",
    "\n",
    "x = np.array([tweet for tweet, label in train_encoded])\n",
    "y = np.array([label for tweet, label in train_encoded])\n",
    "\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(word2index)      \n",
    "dimension_model = 32                             \n",
    "num_layers = 5                       \n",
    "hidden_size = 30                         \n",
    "linear_hidden_size = 10               \n",
    "classes = 7                          \n",
    "dropout = 0.2                            \n",
    "lr = 1e-3      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(src_vocab_size, dimension_model)                        \n",
    "        self.lstm = torch.nn.LSTM(input_size=dimension_model, hidden_size=hidden_size,num_layers=num_layers,dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_size, linear_hidden_size)\n",
    "        self.linear1 = torch.nn.Linear(linear_hidden_size, classes)\n",
    "    def forward(self,data):\n",
    "        x = self.embed(data)                          \n",
    "        x,(h_n, c_n) = self.lstm(x.transpose(0, 1))                                       \n",
    "        x = self.linear(x[-1])                          \n",
    "        x = self.linear1(x)                            \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(sample_size, k=5):\n",
    "    # calculate the size of each fold\n",
    "    fold_size = np.ones(k , dtype=int) * (sample_size // k) \n",
    "    fold_size[:sample_size % k] += 1 # if the sample size is not divisible by k\n",
    "    # shuffle\n",
    "    indexs = np.arange(sample_size)\n",
    "    np.random.default_rng(seed=10).shuffle(indexs)\n",
    "    # split indexs  \n",
    "    folds = np.array_split(indexs, k)\n",
    "    folds = np.array(folds)\n",
    "    # return training index and val index in each fold\n",
    "    ret = []\n",
    "    for i in range(k):\n",
    "        train_fold_ind = np.delete(np.arange(k), i)\n",
    "        train_fold = np.concatenate((folds[train_fold_ind]), axis=None) \n",
    "        val_fold = folds[i]\n",
    "        ret.append([train_fold, val_fold])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 1.532, Train Acc: 46.76%, Val. Loss: 1.413, Val. Acc: 51.63%\n",
      "Epoch: 02, Train Loss: 1.410, Train Acc: 52.19%, Val. Loss: 1.383, Val. Acc: 52.13%\n",
      "Epoch: 03, Train Loss: 1.368, Train Acc: 52.67%, Val. Loss: 1.350, Val. Acc: 52.20%\n",
      "Epoch: 04, Train Loss: 1.320, Train Acc: 53.59%, Val. Loss: 1.328, Val. Acc: 52.56%\n",
      "Epoch: 05, Train Loss: 1.273, Train Acc: 55.62%, Val. Loss: 1.304, Val. Acc: 55.26%\n",
      "Epoch: 01, Train Loss: 1.233, Train Acc: 57.31%, Val. Loss: 1.179, Val. Acc: 58.81%\n",
      "Epoch: 02, Train Loss: 1.187, Train Acc: 58.83%, Val. Loss: 1.198, Val. Acc: 57.95%\n",
      "Epoch: 03, Train Loss: 1.155, Train Acc: 59.97%, Val. Loss: 1.179, Val. Acc: 60.30%\n",
      "Epoch: 04, Train Loss: 1.122, Train Acc: 61.13%, Val. Loss: 1.187, Val. Acc: 58.81%\n",
      "Epoch: 05, Train Loss: 1.089, Train Acc: 62.36%, Val. Loss: 1.172, Val. Acc: 60.37%\n",
      "Epoch: 01, Train Loss: 1.069, Train Acc: 63.08%, Val. Loss: 1.076, Val. Acc: 64.28%\n",
      "Epoch: 02, Train Loss: 1.042, Train Acc: 64.13%, Val. Loss: 1.065, Val. Acc: 64.13%\n",
      "Epoch: 03, Train Loss: 1.018, Train Acc: 65.35%, Val. Loss: 1.077, Val. Acc: 63.64%\n",
      "Epoch: 04, Train Loss: 0.986, Train Acc: 66.23%, Val. Loss: 1.075, Val. Acc: 64.63%\n",
      "Epoch: 05, Train Loss: 0.964, Train Acc: 67.20%, Val. Loss: 1.088, Val. Acc: 63.85%\n",
      "Epoch: 01, Train Loss: 0.956, Train Acc: 68.03%, Val. Loss: 0.935, Val. Acc: 68.04%\n",
      "Epoch: 02, Train Loss: 0.932, Train Acc: 69.22%, Val. Loss: 0.919, Val. Acc: 67.40%\n",
      "Epoch: 03, Train Loss: 0.906, Train Acc: 70.19%, Val. Loss: 0.915, Val. Acc: 67.76%\n",
      "Epoch: 04, Train Loss: 0.885, Train Acc: 70.79%, Val. Loss: 0.926, Val. Acc: 67.26%\n",
      "Epoch: 05, Train Loss: 0.869, Train Acc: 71.35%, Val. Loss: 0.910, Val. Acc: 68.04%\n",
      "Epoch: 01, Train Loss: 0.855, Train Acc: 71.84%, Val. Loss: 0.806, Val. Acc: 72.94%\n",
      "Epoch: 02, Train Loss: 0.838, Train Acc: 72.66%, Val. Loss: 0.809, Val. Acc: 74.01%\n",
      "Epoch: 03, Train Loss: 0.817, Train Acc: 73.42%, Val. Loss: 0.815, Val. Acc: 73.08%\n",
      "Epoch: 04, Train Loss: 0.796, Train Acc: 74.26%, Val. Loss: 0.826, Val. Acc: 73.15%\n",
      "Epoch: 05, Train Loss: 0.778, Train Acc: 74.97%, Val. Loss: 0.820, Val. Acc: 73.58%\n",
      "Epoch: 01, Train Loss: 0.775, Train Acc: 74.99%, Val. Loss: 0.718, Val. Acc: 76.78%\n",
      "Epoch: 02, Train Loss: 0.743, Train Acc: 76.28%, Val. Loss: 0.710, Val. Acc: 77.49%\n",
      "Epoch: 03, Train Loss: 0.726, Train Acc: 76.65%, Val. Loss: 0.715, Val. Acc: 76.56%\n",
      "Epoch: 04, Train Loss: 0.718, Train Acc: 76.76%, Val. Loss: 0.714, Val. Acc: 76.85%\n",
      "Epoch: 05, Train Loss: 0.695, Train Acc: 77.63%, Val. Loss: 0.718, Val. Acc: 76.78%\n",
      "Epoch: 01, Train Loss: 0.699, Train Acc: 77.84%, Val. Loss: 0.638, Val. Acc: 80.33%\n",
      "Epoch: 02, Train Loss: 0.675, Train Acc: 78.60%, Val. Loss: 0.616, Val. Acc: 80.68%\n",
      "Epoch: 03, Train Loss: 0.655, Train Acc: 79.33%, Val. Loss: 0.645, Val. Acc: 79.69%\n",
      "Epoch: 04, Train Loss: 0.643, Train Acc: 79.77%, Val. Loss: 0.667, Val. Acc: 79.76%\n",
      "Epoch: 05, Train Loss: 0.631, Train Acc: 79.88%, Val. Loss: 0.638, Val. Acc: 80.18%\n",
      "Epoch: 01, Train Loss: 0.629, Train Acc: 80.12%, Val. Loss: 0.591, Val. Acc: 81.46%\n",
      "Epoch: 02, Train Loss: 0.612, Train Acc: 80.90%, Val. Loss: 0.589, Val. Acc: 82.10%\n",
      "Epoch: 03, Train Loss: 0.604, Train Acc: 80.97%, Val. Loss: 0.580, Val. Acc: 81.25%\n",
      "Epoch: 04, Train Loss: 0.586, Train Acc: 81.55%, Val. Loss: 0.580, Val. Acc: 82.53%\n",
      "Epoch: 05, Train Loss: 0.582, Train Acc: 81.90%, Val. Loss: 0.589, Val. Acc: 81.53%\n",
      "Epoch: 01, Train Loss: 0.587, Train Acc: 81.78%, Val. Loss: 0.473, Val. Acc: 84.66%\n",
      "Epoch: 02, Train Loss: 0.566, Train Acc: 82.02%, Val. Loss: 0.513, Val. Acc: 84.16%\n",
      "Epoch: 03, Train Loss: 0.569, Train Acc: 82.07%, Val. Loss: 0.521, Val. Acc: 83.95%\n",
      "Epoch: 04, Train Loss: 0.550, Train Acc: 82.80%, Val. Loss: 0.552, Val. Acc: 82.67%\n",
      "Epoch: 05, Train Loss: 0.550, Train Acc: 82.59%, Val. Loss: 0.521, Val. Acc: 84.30%\n",
      "Epoch: 01, Train Loss: 0.542, Train Acc: 82.94%, Val. Loss: 0.452, Val. Acc: 86.15%\n",
      "Epoch: 02, Train Loss: 0.529, Train Acc: 83.45%, Val. Loss: 0.437, Val. Acc: 87.00%\n",
      "Epoch: 03, Train Loss: 0.516, Train Acc: 83.57%, Val. Loss: 0.459, Val. Acc: 86.15%\n",
      "Epoch: 04, Train Loss: 0.511, Train Acc: 83.51%, Val. Loss: 0.457, Val. Acc: 85.94%\n",
      "Epoch: 05, Train Loss: 0.500, Train Acc: 84.10%, Val. Loss: 0.471, Val. Acc: 86.15%\n"
     ]
    }
   ],
   "source": [
    "model = LSTM().to(device)                           \n",
    "criterion = torch.nn.CrossEntropyLoss()                  \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr) \n",
    "\n",
    "best_acc = 0\n",
    "kfold_index = cross_validation(sample_size = x.shape[0], k=10)\n",
    "for train_index, val_index in kfold_index:\n",
    "    train_x = x[train_index]\n",
    "    train_y = y[train_index]\n",
    "    dev_x = x[val_index]\n",
    "    dev_y = y[val_index]\n",
    "    train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    dev_ds = TensorDataset(torch.from_numpy(dev_x), torch.from_numpy(dev_y))\n",
    "\n",
    "    train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    dev_dl = DataLoader(dev_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        train_loss, train_acc = train(model, train_dl, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, dev_dl, criterion)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc * 100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc * 100:.2f}%')    \n",
    "        if best_acc <= valid_acc:\n",
    "            best_acc = valid_acc\n",
    "            PATH=f\"epoch{epoch+1}_val.accuracy{valid_loss:.3f}%.pt\"\n",
    "            torch.save({\n",
    "                    'epoch': epoch+1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': valid_loss,\n",
    "                    }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_HW2dataset.csv')\n",
    "test_df = test_df[['Utterance']]\n",
    "test_set = test_df.values.tolist()\n",
    "test_encoded=[]\n",
    "for sentence in test_set:\n",
    "    test_encoded += [encode_test(Utterance, word2index, 10) for Utterance in sentence]\n",
    "test_x = np.array(test_encoded)\n",
    "test_ds = TensorDataset(torch.from_numpy(test_x))\n",
    "test_dl = DataLoader(test_ds, shuffle=False)\n",
    " \n",
    "model = LSTM().to(device)                               \n",
    "criterion = torch.nn.CrossEntropyLoss()                  \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr) \n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()\n",
    "predict=[]\n",
    "for deta in test_dl:\n",
    "    text = deta[0].to(device)\n",
    "    preds = model(text)\n",
    "    _, pred = torch.max(preds, 1)\n",
    "    predict.append(pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.DataFrame(columns=[\"index\",\"emotion\"])\n",
    "for ind, p in enumerate(predict):\n",
    "    ans.loc[ind] = [ind,p]\n",
    "ans.to_csv(\"version1.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
