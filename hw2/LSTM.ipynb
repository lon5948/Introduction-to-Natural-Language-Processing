{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import digits\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str) -> str:\n",
    "    stop_word_list = stopwords.words('english')\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_word_list]\n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "def lemmatization(text: str) -> str:\n",
    "    tokens = word_tokenize(text)\n",
    "    wl = WordNetLemmatizer()\n",
    "    lemma_tokens= []\n",
    "    for token in text.split():\n",
    "        word1 = wl.lemmatize(token,pos = \"n\")\n",
    "        word2 = wl.lemmatize(word1,pos = \"v\")\n",
    "        word3 = wl.lemmatize(word2,pos = \"a\")\n",
    "        word4 = wl.lemmatize(word3,pos = \"r\")\n",
    "        lemma_tokens.append(word4)\n",
    "    preprocessed_text = ' '.join(lemma_tokens)\n",
    "    return preprocessed_text\n",
    "    \n",
    "def remove_punctuation(text: str) -> str:\n",
    "    tokens = [token for token in text if token not in punctuation]\n",
    "    preprocessed_text = ''.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "def remove_digit(text: str) -> str:\n",
    "    tokens = [token for token in text if token not in digits]\n",
    "    preprocessed_text = ''.join(tokens)\n",
    "    return preprocessed_text\n",
    "    \n",
    "def preprocessing_function(text: str) -> str:\n",
    "    preprocess_text = text.lower()\n",
    "    #preprocessed_text = lemmatization(text)\n",
    "    #preprocessed_text = remove_stopwords(preprocessed_text)\n",
    "    #preprocessed_text = remove_punctuation(preprocessed_text)\n",
    "    #preprocessed_text = remove_digit(preprocessed_text)\n",
    "    return preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(label):\n",
    "    if label == \"neutral\":\n",
    "        return 0\n",
    "    elif label == \"anger\":\n",
    "        return 1\n",
    "    elif label == \"joy\":\n",
    "        return 2\n",
    "    elif label == \"surprise\":\n",
    "        return 3\n",
    "    elif label == \"sadness\":\n",
    "        return 4\n",
    "    elif label == \"disgust\":\n",
    "        return 5\n",
    "    elif label == \"fear\":\n",
    "        return 6\n",
    "\n",
    "def man_map(speaker):\n",
    "    if speaker == \"Ross\":\n",
    "        return 1\n",
    "    elif speaker == \"Joey\":\n",
    "        return 2\n",
    "    elif speaker == \"Rachel\":\n",
    "        return 3\n",
    "    elif speaker == \"Phoeba\":\n",
    "        return 4\n",
    "    elif speaker == \"Monica\":\n",
    "        return 5\n",
    "    elif speaker == \"Chandler\":\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "\n",
    "def encode(text, word2index, label, speaker, N):\n",
    "    tokenized = word_tokenize(text)\n",
    "    encoded = [0]*N\n",
    "    enc1 = [word2index.get(word) for word in tokenized]\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    #encoded.insert(man_map(speaker) ,0)\n",
    "    return (encoded,label)\n",
    "\n",
    "def encode_test(text, word2index, speaker, N):\n",
    "    tokenized = word_tokenize(text)\n",
    "    for i,word in enumerate(tokenized):\n",
    "        if word2index.get(word)==None:\n",
    "            tokenized[i]='unk'\n",
    "    encoded = [0]*N\n",
    "    enc1 = [word2index.get(word) for word in tokenized]\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    #encoded.insert(man_map(speaker) ,0)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_HW2dataset.csv')\n",
    "dev_df = pd.read_csv('dev_HW2dataset.csv')\n",
    "\n",
    "train_df = train_df[['Emotion','Utterance','Speaker']]\n",
    "dev_df = dev_df[['Emotion','Utterance','Speaker']]\n",
    "\n",
    "train_set = list(train_df.to_records(index=False))\n",
    "dev_set = list(dev_df.to_records(index=False))\n",
    "\n",
    "counts = Counter()\n",
    "new_set = []\n",
    "for ds in [train_set, dev_set]:\n",
    "    for label,text,speaker in ds:\n",
    "        text = preprocessing_function(text)\n",
    "        new_set.append((label,text,speaker))\n",
    "        counts.update(word_tokenize(text))\n",
    "\n",
    "word2index = {'unk':0}\n",
    "for i,word in enumerate(counts.keys()):\n",
    "    word2index[word] = i+1\n",
    "index2word = {v:k for k,v in word2index.items()}\n",
    "\n",
    "emb_size = 300\n",
    "c = 0\n",
    "pretrained_weight = np.zeros((len(word2index), emb_size), dtype=\"float32\")\n",
    "\n",
    "with open('glove.840B.300d.txt','rt',encoding=\"utf-8\") as fi:\n",
    "    for line in fi:\n",
    "        i_word = line.split(' ')[0]\n",
    "        i_embeddings = [float(val) for val in line.split(' ')[1:]]\n",
    "        if i_word in word2index:\n",
    "            pretrained_weight[word2index[i_word]] = i_embeddings\n",
    "            c += 1\n",
    "\n",
    "new_encoded = [(encode(Utterance,word2index,label_map(label), speaker, 20)) for label, Utterance, speaker in new_set]\n",
    "\n",
    "x = np.array([tweet for tweet, label in new_encoded])\n",
    "y = np.array([label for tweet, label in new_encoded])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, target)\n",
    "        _, pred = torch.max(preds, 1)\n",
    "        acc = accuracy_score(pred.tolist(), target.tolist())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, target)\n",
    "        _, pred = torch.max(preds, 1)\n",
    "        acc = accuracy_score(pred.tolist(), target.tolist())\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(word2index)     \n",
    "dimension_model = 300                             \n",
    "num_layers = 5                       \n",
    "hidden_size = 256          \n",
    "linear_hidden_size = 32   \n",
    "classes = 7                          \n",
    "dropout = 0.2                          \n",
    "lr = 1e-3      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(sample_size, k=5):\n",
    "    # calculate the size of each fold\n",
    "    fold_size = np.ones(k , dtype=int) * (sample_size // k) \n",
    "    fold_size[:sample_size % k] += 1 # if the sample size is not divisible by k\n",
    "    # shuffle\n",
    "    indexs = np.arange(sample_size)\n",
    "    np.random.default_rng().shuffle(indexs)\n",
    "    # split indexs  \n",
    "    folds = np.array_split(indexs, k)\n",
    "    folds = np.array(folds)\n",
    "    # return training index and val index in each fold\n",
    "    ret = []\n",
    "    for i in range(k):\n",
    "        train_fold_ind = np.delete(np.arange(k), i)\n",
    "        train_fold = np.concatenate((folds[train_fold_ind]), axis=None) \n",
    "        val_fold = folds[i]\n",
    "        ret.append([train_fold, val_fold])\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        #self.embed = torch.nn.Embedding(src_vocab_size, dimension_model) \n",
    "        self.embed = torch.nn.Embedding.from_pretrained(torch.from_numpy(pretrained_weight).float())                       \n",
    "        self.lstm = torch.nn.LSTM(input_size=dimension_model, hidden_size=hidden_size,\n",
    "                                    num_layers=num_layers,dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_size, linear_hidden_size)\n",
    "        self.fc = torch.nn.Linear(linear_hidden_size, classes)\n",
    "    \n",
    "    def attention(self, lstm_output, final_state):\n",
    "        lstm_output = lstm_output.permute(1, 0, 2) \n",
    "        merged_state = torch.mean(final_state, dim=0) \n",
    "        merged_state = merged_state.unsqueeze(2)\n",
    "        weights = torch.bmm(lstm_output, merged_state)\n",
    "        weights = torch.nn.functional.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        ret = torch.bmm(torch.transpose(lstm_output, 1, 2), weights).squeeze(2)\n",
    "        return ret\n",
    "\n",
    "    def forward(self,data):\n",
    "        x = self.embed(data)                    \n",
    "        x,(h_n, c_n) = self.lstm(x.transpose(0, 1)) \n",
    "        x = self.attention(x, h_n) \n",
    "        x = self.linear(x)  \n",
    "        x = self.fc(x)                          \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 1.498, Train Acc: 49.18%, Val. Loss: 1.393, Val. Acc: 52.84%\n",
      "Epoch: 02, Train Loss: 1.373, Train Acc: 53.27%, Val. Loss: 1.323, Val. Acc: 54.62%\n",
      "Epoch: 03, Train Loss: 1.306, Train Acc: 55.74%, Val. Loss: 1.268, Val. Acc: 56.53%\n",
      "Epoch: 04, Train Loss: 1.239, Train Acc: 58.36%, Val. Loss: 1.230, Val. Acc: 57.74%\n",
      "Epoch: 05, Train Loss: 1.189, Train Acc: 60.40%, Val. Loss: 1.204, Val. Acc: 58.66%\n",
      "Epoch: 01, Train Loss: 1.501, Train Acc: 47.92%, Val. Loss: 1.446, Val. Acc: 49.22%\n",
      "Epoch: 02, Train Loss: 1.384, Train Acc: 52.31%, Val. Loss: 1.363, Val. Acc: 53.76%\n",
      "Epoch: 03, Train Loss: 1.324, Train Acc: 54.27%, Val. Loss: 1.318, Val. Acc: 53.84%\n",
      "Epoch: 04, Train Loss: 1.285, Train Acc: 56.25%, Val. Loss: 1.283, Val. Acc: 55.54%\n",
      "Epoch: 05, Train Loss: 1.244, Train Acc: 57.49%, Val. Loss: 1.244, Val. Acc: 56.61%\n",
      "Epoch: 01, Train Loss: 1.568, Train Acc: 45.83%, Val. Loss: 1.572, Val. Acc: 45.31%\n",
      "Epoch: 02, Train Loss: 1.447, Train Acc: 50.75%, Val. Loss: 1.366, Val. Acc: 55.40%\n",
      "Epoch: 03, Train Loss: 1.325, Train Acc: 55.82%, Val. Loss: 1.296, Val. Acc: 56.53%\n",
      "Epoch: 04, Train Loss: 1.270, Train Acc: 57.30%, Val. Loss: 1.304, Val. Acc: 57.03%\n",
      "Epoch: 05, Train Loss: 1.228, Train Acc: 58.67%, Val. Loss: 1.262, Val. Acc: 58.31%\n",
      "Epoch: 01, Train Loss: 1.567, Train Acc: 45.52%, Val. Loss: 1.547, Val. Acc: 46.45%\n",
      "Epoch: 02, Train Loss: 1.507, Train Acc: 47.96%, Val. Loss: 1.365, Val. Acc: 53.62%\n",
      "Epoch: 03, Train Loss: 1.368, Train Acc: 53.30%, Val. Loss: 1.360, Val. Acc: 53.98%\n",
      "Epoch: 04, Train Loss: 1.333, Train Acc: 53.39%, Val. Loss: 1.301, Val. Acc: 54.62%\n",
      "Epoch: 05, Train Loss: 1.285, Train Acc: 55.08%, Val. Loss: 1.266, Val. Acc: 56.61%\n",
      "Epoch: 01, Train Loss: 1.566, Train Acc: 45.87%, Val. Loss: 1.589, Val. Acc: 44.32%\n",
      "Epoch: 02, Train Loss: 1.559, Train Acc: 46.04%, Val. Loss: 1.591, Val. Acc: 44.39%\n",
      "Epoch: 03, Train Loss: 1.537, Train Acc: 46.44%, Val. Loss: 1.479, Val. Acc: 47.09%\n",
      "Epoch: 04, Train Loss: 1.359, Train Acc: 54.09%, Val. Loss: 1.334, Val. Acc: 54.55%\n",
      "Epoch: 05, Train Loss: 1.290, Train Acc: 56.81%, Val. Loss: 1.323, Val. Acc: 55.89%\n",
      "Epoch: 01, Train Loss: 1.447, Train Acc: 50.12%, Val. Loss: 1.407, Val. Acc: 53.41%\n",
      "Epoch: 02, Train Loss: 1.339, Train Acc: 53.31%, Val. Loss: 1.311, Val. Acc: 54.97%\n",
      "Epoch: 03, Train Loss: 1.281, Train Acc: 56.13%, Val. Loss: 1.261, Val. Acc: 57.88%\n",
      "Epoch: 04, Train Loss: 1.219, Train Acc: 58.48%, Val. Loss: 1.198, Val. Acc: 59.23%\n",
      "Epoch: 05, Train Loss: 1.167, Train Acc: 61.14%, Val. Loss: 1.159, Val. Acc: 61.29%\n",
      "Epoch: 01, Train Loss: 1.484, Train Acc: 48.60%, Val. Loss: 1.412, Val. Acc: 52.06%\n",
      "Epoch: 02, Train Loss: 1.363, Train Acc: 53.46%, Val. Loss: 1.333, Val. Acc: 54.47%\n",
      "Epoch: 03, Train Loss: 1.292, Train Acc: 55.68%, Val. Loss: 1.273, Val. Acc: 56.82%\n",
      "Epoch: 04, Train Loss: 1.230, Train Acc: 58.15%, Val. Loss: 1.236, Val. Acc: 59.30%\n",
      "Epoch: 05, Train Loss: 1.178, Train Acc: 60.84%, Val. Loss: 1.229, Val. Acc: 58.95%\n",
      "Epoch: 01, Train Loss: 1.575, Train Acc: 45.39%, Val. Loss: 1.487, Val. Acc: 48.51%\n",
      "Epoch: 02, Train Loss: 1.493, Train Acc: 48.12%, Val. Loss: 1.320, Val. Acc: 56.61%\n",
      "Epoch: 03, Train Loss: 1.342, Train Acc: 54.89%, Val. Loss: 1.257, Val. Acc: 58.45%\n",
      "Epoch: 04, Train Loss: 1.311, Train Acc: 55.60%, Val. Loss: 1.236, Val. Acc: 58.88%\n",
      "Epoch: 05, Train Loss: 1.259, Train Acc: 57.68%, Val. Loss: 1.203, Val. Acc: 61.01%\n",
      "Epoch: 01, Train Loss: 1.445, Train Acc: 50.49%, Val. Loss: 1.406, Val. Acc: 51.42%\n",
      "Epoch: 02, Train Loss: 1.330, Train Acc: 54.52%, Val. Loss: 1.343, Val. Acc: 54.47%\n",
      "Epoch: 03, Train Loss: 1.273, Train Acc: 56.59%, Val. Loss: 1.290, Val. Acc: 56.04%\n",
      "Epoch: 04, Train Loss: 1.218, Train Acc: 58.88%, Val. Loss: 1.261, Val. Acc: 57.60%\n",
      "Epoch: 05, Train Loss: 1.160, Train Acc: 60.89%, Val. Loss: 1.203, Val. Acc: 60.87%\n",
      "Epoch: 01, Train Loss: 1.512, Train Acc: 48.13%, Val. Loss: 1.419, Val. Acc: 53.41%\n",
      "Epoch: 02, Train Loss: 1.398, Train Acc: 53.02%, Val. Loss: 1.388, Val. Acc: 52.91%\n",
      "Epoch: 03, Train Loss: 1.359, Train Acc: 53.33%, Val. Loss: 1.354, Val. Acc: 53.55%\n",
      "Epoch: 04, Train Loss: 1.323, Train Acc: 54.49%, Val. Loss: 1.294, Val. Acc: 57.32%\n",
      "Epoch: 05, Train Loss: 1.261, Train Acc: 56.89%, Val. Loss: 1.249, Val. Acc: 57.53%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold_data = cross_validation(sample_size = x.shape[0], k=10)\n",
    "batch_size = 32\n",
    "\n",
    "for train_index, val_index in kfold_data:\n",
    "    train_x = x[train_index]\n",
    "    train_y = y[train_index]\n",
    "    dev_x = x[val_index]\n",
    "    dev_y = y[val_index]\n",
    "    train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    dev_ds = TensorDataset(torch.from_numpy(dev_x), torch.from_numpy(dev_y))\n",
    "\n",
    "    train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    dev_dl = DataLoader(dev_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    best_acc = 0\n",
    "    model = LSTM().to(device)                           \n",
    "    criterion = torch.nn.CrossEntropyLoss()                  \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    for epoch in range(5):\n",
    "        train_loss, train_acc = train(model, train_dl, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, dev_dl, criterion)\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc * 100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc * 100:.2f}%')    \n",
    "        if best_acc <= valid_acc:\n",
    "            best_acc = valid_acc\n",
    "            PATH = f\"epoch{epoch+1}_val.accuracy{valid_loss:.3f}%.pt\"\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': valid_loss,\n",
    "                }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_HW2dataset.csv')\n",
    "test_df = test_df[['Utterance','Speaker']]\n",
    "test_set = test_df.values.tolist()\n",
    "\n",
    "new_test_set = []\n",
    "for ds in [test_set]:\n",
    "    for text,speaker in ds:\n",
    "        text = preprocessing_function(text)\n",
    "        new_test_set.append((text,speaker))\n",
    "\n",
    "test_encoded = [(encode_test(Utterance, word2index, speaker, 18)) for Utterance, speaker in new_test_set]\n",
    "test_x = np.array(test_encoded)\n",
    "test_ds = TensorDataset(torch.from_numpy(test_x))\n",
    "test_dl = DataLoader(test_ds, shuffle=False)\n",
    "model = LSTM().to(device)                               \n",
    "criterion = torch.nn.CrossEntropyLoss()                  \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr) \n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()\n",
    "predict=[]\n",
    "for deta in test_dl:\n",
    "    text = deta[0].to(device)\n",
    "    preds = model(text)\n",
    "    _, pred = torch.max(preds, 1)\n",
    "    predict.append(pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27440\\63181726.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"emotion\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predict.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "ans = pd.DataFrame(columns=[\"index\",\"emotion\"])\n",
    "for ind, p in enumerate(predict):\n",
    "    ans.loc[ind] = [ind,p]\n",
    "ans.to_csv(\"predict.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
